{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed3a357",
   "metadata": {},
   "source": [
    "## Numerical Methods\n",
    "\n",
    "### What is Numerical Methods? And where are these methods applicable?\n",
    "\n",
    "Techniques for solving large-scale linear systems and for finding numerical approximations of various kinds!  \n",
    "\n",
    "=> Main Contemporary Application : Singular Value Decomposition(SVD) And Data Compression\n",
    "\n",
    "\n",
    "### Chapter Contents\n",
    "\n",
    "- LU-Decomposition\n",
    "- The Power Method\n",
    "- Comparison of Procedures for Solving Linear Systems\n",
    "- Singular Value Decomposition\n",
    "- Data Compressing Using Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64a1a90",
   "metadata": {},
   "source": [
    "## Contents Sumamry\n",
    "\n",
    "# 1. LU-Decomposition\n",
    "\n",
    "LU decomposition is a core tool for solving linear problems in numerical analysis.  \n",
    "In particular, it is much faster and more stable than Gaussian elimination  \n",
    "when solving large-scale systems or problems that require iterative computation.\n",
    "\n",
    "  Gaussian: N($O^3$)  \n",
    "  LU-D: N($O^2$)\n",
    "\n",
    "---\n",
    "\n",
    "**LU 분해를 이용한 선형 시스템 풀이 요약**\n",
    "\n",
    "선형 시스템:  \n",
    "$A \\vec{x} = \\vec{b}$\n",
    "\n",
    "LU 분해:  \n",
    "$A = LU \\Rightarrow LU \\vec{x} = \\vec{b}$\n",
    "\n",
    "중간값 $\\vec{y}$를 도입하여 두 단계로 나누어 계산:\n",
    "\n",
    "---\n",
    "\n",
    "**Step 1: 전진 대입 (Forward Substitution)**\n",
    "\n",
    "$L \\vec{y} = \\vec{b}$\n",
    "\n",
    "- 아래 삼각행렬 $L$ 사용\n",
    "- $y_1 \\to y_2 \\to \\cdots \\to y_n$ 순서로 **위에서 아래로** 계산\n",
    "\n",
    "---\n",
    "\n",
    "**Step 2: 후진 대입 (Backward Substitution)**\n",
    "\n",
    "$U \\vec{x} = \\vec{y}$\n",
    "\n",
    "- 위 삼각행렬 $U$ 사용\n",
    "- $x_n \\to x_{n-1} \\to \\cdots \\to x_1$ 순서로 **아래에서 위로** 계산\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df875be",
   "metadata": {},
   "source": [
    "# 2. The Power Method\n",
    "\n",
    "## **Background**\n",
    "\n",
    "**[Definition of Eigenvalues and Eigenvectors]**\n",
    "\n",
    "In the field of linear Algebra, 'eigenvalues' and 'eivenvectors' are essential. First, to briefly recall     definition: v is an eigenvector and λ is an eigenvalue if Av =  λv.  \n",
    "\n",
    "In other words, if a matrix A only scales a vector(without changing its direction), then the vector is an eigenvector of matrix A and, and the scaling factor is the corresponding eigenvalue. \n",
    "\n",
    "Every square matrix has eigenvalues and eigenvectors over the complex numbers. Moreover, if a matrix is symmetric, it always has real eigenvalues. However, some matrices cannot be diagonized due to a lack of independent eigenvectors.  \n",
    "\n",
    "| 행렬의 종류                | 고유값          | 고유벡터               | 설명               |\n",
    "| --------------------- | ------------ | ------------------ | ---------------- |\n",
    "| 일반 정사각행렬              | ✅ (복소수에서 항상) | ✅ (복소수에서 항상)       | 특성방정식의 근이므로 존재   |\n",
    "| 실수 정사각행렬              | ❌ 항상 실수는 아님  | ❌ 실수 고유벡터도 아닐 수 있음 | 복소수까지 확장 필요      |\n",
    "| 대칭행렬 (real symmetric) | ✅ 항상 실수      | ✅ 서로 직교            | 좋은 성질: 항상 대각화 가능 |\n",
    "| 비정규 행렬                | ✅ 있음         | ❌ 선형독립 고유벡터 부족 가능  | Jordan Form 필요   |\n",
    "\n",
    "\n",
    "**[The importance of Eigenvalues and EigenVectors]**  \n",
    "                                                          \n",
    "Intuitively, an eigenvector indicates the diretion in which a matrix transforms a vector without changing its orientation. An eigenvalue represents how much the vector is stretched or compressed along its eigenvector.  \n",
    "\n",
    "Eigenvector: direction  \n",
    "\n",
    "Eigenvalue: scaling factor or magnitude of the effect  \n",
    "\n",
    "λ>1 → Exponential growth in that direction\n",
    "\n",
    "λ<1 → Convergence or Decay\n",
    "\n",
    "λ=1 → Steady state\n",
    "\n",
    "λ<0 → Oscillation or direction reversal  \n",
    "\n",
    "**Note 1:** In data science, PCA uses eigenvectors to identify the principal directions (components) of the data, and in search engines, PageRank leverages eigenvectors to measure the relative importance of web pages.  \n",
    "\n",
    "**Note 2:** Complex eigenvalues usually indicate rotational or spiral dynamics in the transformation.  \n",
    "λ=r $e^{i\\theta}$ \n",
    "where **r** is the magnitude (modulus), and **θ** is the angle(argument)\n",
    "\n",
    "**[Challenges in finding eigenvalues and eigenvectors]**\n",
    "\n",
    "In theory, the eigenvalues of a square matrix can be obtained by solving the characteristic equation. However, in practice, this method is rarely used due to computational difficulties. The Power method is a practical tool as a practical alternative.\n",
    "\n",
    "## **Power Method: Algorithm and Why It Works**\n",
    "\n",
    "### Algorithm Steps\n",
    "\n",
    "1. Choose an initial non-zero vector $ \\vec{x}_0 $.  \n",
    "   It must have a non-zero component in the direction of the dominant eigenvector $ \\vec{v}_1 $.\n",
    "\n",
    "2. Repeat:\n",
    "   - Multiply: $ \\vec{x}_{k+1} = A \\vec{x}_k $\n",
    "   - Normalize:  \n",
    "     $ \\vec{x}_{k+1} := \\frac{\\vec{x}_{k+1}}{\\|\\vec{x}_{k+1}\\|} $\n",
    "\n",
    "3. Continue until convergence, i.e., $ \\vec{x}_{k+1} \\approx \\vec{x}_k $.\n",
    "\n",
    "4. (Optional) Estimate the dominant eigenvalue:  \n",
    "   $$ \\lambda_1 \\approx \\frac{\\vec{x}_k^T A \\vec{x}_k}{\\vec{x}_k^T \\vec{x}_k} $$\n",
    "\n",
    "---\n",
    "\n",
    "### Why It Converges to the Dominant Eigenvector\n",
    "\n",
    "Let $ A $ have eigenvalues:\n",
    "\n",
    "$$\n",
    "|\\lambda_1| > |\\lambda_2| \\ge |\\lambda_3| \\ge \\cdots \\ge |\\lambda_n|\n",
    "$$\n",
    "\n",
    "Assume the initial vector is expressed as:\n",
    "\n",
    "$$\n",
    "\\vec{x}_0 = c_1 \\vec{v}_1 + c_2 \\vec{v}_2 + \\cdots + c_n \\vec{v}_n\n",
    "$$\n",
    "\n",
    "Then applying $ A $ repeatedly:\n",
    "\n",
    "$$\n",
    "A^k \\vec{x}_0 = c_1 \\lambda_1^k \\vec{v}_1 + c_2 \\lambda_2^k \\vec{v}_2 + \\cdots + c_n \\lambda_n^k \\vec{v}_n\n",
    "$$\n",
    "\n",
    "As $ k \\to \\infty $, the term with $ \\lambda_1^k $ dominates:\n",
    "\n",
    "$$\n",
    "A^k \\vec{x}_0 \\approx c_1 \\lambda_1^k \\vec{v}_1\n",
    "$$\n",
    "\n",
    "After normalization, $ \\vec{x}_k $ converges to the direction of $ \\vec{v}_1 $,  \n",
    "the eigenvector corresponding to the largest (absolute) eigenvalue.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
